# 参考

https://www.zhihu.com/question/28641663

# 特征离散化（分箱）
一般类别变量需要离散化处理.
类别变量再细分可以分成有序变量和无序变量，典型的有序变量就是学历，如博士研究生，硕士研究生，本科生等在业务含义上本身是有高低之分的。而无序变量在业务含义上是无序的，如品牌等.



有的时候需要将string等特征转换成数值特征.
转换策略大概分成两类embeding以及稀疏表示。

1. embeding. 
    * 代表就是label encoder(直接编码成类别需要)，缺点是强制规定了顺序，因此对于一些需要计算特征距离or大小顺序的模型中就有比较大的影响。
    * 一般模型的损失函数中对数值大小敏感，例如逻辑回归，SVM会计算点到超平面的距离，此时使用embeding就问题比较大.
    * 缺点就是，当类别的数量很多时，特征空间会变得非常大。这里有一些经验上的处理方式。优先考虑的是有没业务上类别合并的方法，如城市变量，可以依发展程度分为一线城市，二线城市等等；另外一种方法是只one hot出现次数最多的前n个类别，其他类别放在其他类的变量中；也可以利用y值（训练集）中positive rate做合并，不过容易出现过拟合的现象。

2. 稀疏表示. 代表是one-hot, 编码成离散的二进制数字，缺点是类别多的时候会导致特征维度过高，模型效果就比较差。
    * 一般类别之间没有线性关系就应该 one hot 编码.
    * 树模型一般情况不建议用ont-hot，会导致的树的深度过深和树的生长不平衡。


## gbdt

高维稀疏特征的时候，使用 gbdt 很容易过拟合.
后来思考后发现原因是因为现在的模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面稀疏的特征，树只需要一个节点就可以完美分割9990和10个样本(因为onehot每个维度就只有0/1)，惩罚项极其之小.这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。



lightgbm内置了处理离散特征的方法, 会自动将其映射为整数，在节点分类的时候按照相应类别尝试不同的切分点.
xgboost之类的就需要自己特殊处理.


树模型一般情况不建议用ont-hot，会导致的树的深度过深和树的生长不平衡。
举个例子，在树模型中，有四个类别，dog,cat,mouse,horse，如果dog类的占比很少，但它的positive rate与cat是类似的，若在one hot encoding中，前三个类别都入模型了。如果我们做label encoding时，随机将dog,cat,mouse,horse分别赋值为[1,3,2,4]，那么可以预见这个变量会分裂两次，达到上面one hot encoding的效果。但是如果我们将dog,cat,mouse赋值为[1,2,3,4]，可能这个变量只会分裂1次，它将dog和cat分在一起，此时达到的效果可能与one hot encoding相当。

但是在实际使用中，由于one hot encoding增加了变量的维度，在树模型中意味着更深的分裂，如果刚好树的深度限制在一数值时(正则化)，dog这个类别可能不能入模型，其实意味着丢失了dog这一信息.



除了楼上说的lightGBM能直接支持类别特征，leave-one-out方法也还不错，有兴趣可以看看下面的介绍.
