
# æ€»ç»“
ç›¸æ¯”RNNï¼Œattentionæ¶æ„çš„ä¼˜ç‚¹åœ¨äº
RNN å›ºæœ‰çš„é¡ºåºå±æ€§é˜»ç¢äº†è®­ç»ƒæ ·æœ¬é—´çš„å¹¶è¡ŒåŒ–ï¼ŒRNN is aligning the positions to steps in computation time, å¯¹äºé•¿åºåˆ—ï¼Œå†…å­˜é™åˆ¶å°†é˜»ç¢å¯¹è®­ç»ƒæ ·æœ¬çš„æ‰¹é‡å¤„ç†(å› ä¸ºRNNä¸èƒ½å†timeä¸Šå¹¶è¡Œï¼Œåªèƒ½å†æ‰¹æ¬¡ä¸Šæé«˜å¹¶è¡Œï¼Œä½†æ˜¯å†…å­˜é™åˆ¶ï¼Œæ‰¹æ¬¡ä¸èƒ½å¾ˆå¤§ï¼‰ã€‚
Transformerï¼Œæ˜¯ä¸€ç§é¿å…å¾ªç¯(recurrent)çš„æ¨¡å‹ç»“æ„ï¼Œå®Œå…¨ä¾èµ–äºæ³¨æ„åŠ›æœºåˆ¶å¯¹è¾“å…¥è¾“å‡ºçš„å…¨å±€ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚



attentionç”¨äºæ—¶åºæ•°æ®çš„ä¸‹é¢ä¸‰ä¸ªä¸ªé˜»ç¢

1. self-attentionå¯¹ä½ç½®ä¸æ•æ„Ÿï¼Œå› ä¸ºç®—scoreçš„æ—¶å€™æ˜¯æ ¹æ®æ‰€æœ‰ä½ç½®çš„ä¿¡æ¯è®¡ç®—ï¼Œæœ€åçš„åŠ æƒæ±‚å’Œä¹Ÿå¯¹ä½ç½®ä¸æ•æ„Ÿï¼Œæ„å‘³ç€äº¤æ¢æŸäº›è¾“å…¥çš„ä½ç½®ï¼Œè¾“å‡ºä¸å˜ã€‚
2. self-attentionä¸­ä¸åŒ…æ‹¬éçº¿æ€§å˜æ¢ï¼ŒNo nonlinearities for deep learning! Itâ€™s all just weighted averages.
3. éœ€è¦ä¿è¯decoderçš„attentionä¸è¯»å–futureçš„ä¿¡æ¯ï¼Œå¯ä»¥æŠŠkey,value setä¸­å»é™¤futureï¼Œä½†æ˜¯

å› æ­¤attentionå¤„ç†æ—¶åºçš„æ—¶å€™éœ€è¦ä¸‰ä¸ªè¦ç´ , position embeding, æ·»åŠ éçº¿æ€§ï¼Œfuture mask
   


![](pic/2021-04-17-20-57-52.png)

ä¸Šé¢å¯¹attentionçš„ä¼˜åŒ–è¿˜ä¸å¤Ÿ, Transformerè¿˜åšäº†ä¸‹é¢çš„ä¸€äº›æ”¹åŠ¨
1. KQVå½¢å¼çš„self-attentionï¼Œä¹‹å‰ç”¨çš„attentionè¿˜æ˜¯å¤ªç®€å•äº†
2. Multi-headed attention: Attend to multiple places in a single layer!
3. training trick
   * Residual connections2
   * Layer normalization3
   * Scaling the dot product

KQV selfattentionå¯ä»¥è¡¨ç¤ºä¸ºä¸‹é¢å½¢å¼

![](pic/2021-04-18-09-01-47.png)

ç±»æ¯”CNNä¸­åŒæ—¶ä½¿ç”¨å¤šä¸ªæ»¤æ³¢å™¨çš„ä½œç”¨ï¼Œç›´è§‚ä¸Šè®²ï¼Œå¤šå¤´çš„æ³¨æ„åŠ›æœ‰åŠ©äºç½‘ç»œæ•æ‰åˆ°æ›´ä¸°å¯Œçš„ç‰¹å¾/ä¿¡æ¯ã€‚

![](pic/2021-04-18-09-49-03.png)

ä¸ºäº†æé«˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œéœ€è¦è¶³å¤Ÿçš„æ¨¡å‹å®¹é‡ï¼Œä¹Ÿå°±éœ€è¦hidden sizeè¶³å¤Ÿå¤§ã€‚ä½†æ˜¯å¦‚æœä¸æ‹†åˆ†æˆå¤šå¤´ï¼Œé‚£ä¹ˆæ¨¡å‹å°±è¿‡äºå¤æ‚äº†ã€‚æ‰€ä»¥å¤šå¤´çš„å¤§hidden sizeå®é™…ä¸Šå°±æ˜¯å¤æ‚åº¦å’Œå®¹é‡çš„å¹³è¡¡ã€‚å¤´æ•°çš„å¥½å¤„å°±æ˜¯ä¸ºå¹³è¡¡æ¨¡å‹æä¾›äº†ä¸€ä¸ªå¤šä½™çš„è°ƒå‚ç©ºé—´ã€‚è¿™ä¹Ÿè§£é‡Šäº†å¤´æ•°ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œä¹Ÿä¸æ˜¯è¶Šå°‘è¶Šå¥½ï¼Œè€Œæ˜¯éœ€è¦å¹³è¡¡ã€‚
æ‰€ä»¥è¿™é‡Œçš„multi-head attentionä½œç”¨æ˜¯ 1.å¹¶è¡ŒåŒ– å¹¶ä¸æ˜¯å’Œ head=1å»æ¯”è¾ƒçš„ï¼Œè€Œæ˜¯å’Œå•çº¯çš„stacking more layer å»æ¯”è¾ƒ 2. æ‰æ˜¯ å’Œhead=1å»æ¯”è¾ƒï¼Œmulti-head å¯ä»¥å¢åŠ ç½‘ç»œçš„capacity

Residual connectionsæœ¬è´¨æ˜¯æ‰“ç ´äº†å±‚çº§ç»“æ„çš„æ˜¾ç¤ºï¼Œä½¿å¾—ä¸‹å±‚ä¿¡æ¯æ›´å®¹æ˜“ä¼ é€’åˆ°ä¸Šå±‚ã€‚
Residual connections are thought to makethe loss landscape considerably smoother(thuseasier training!)

![](pic/2021-04-18-19-24-21.png)

When dimensionality ğ‘‘ becomes large, dot products between vectors tend to become large.
inputs to the softmax function can be large, è¿™å¾ˆå¯èƒ½ä½¿å¾—softmaxåˆ°è¾¾é¥±å’ŒåŒºï¼Œæ˜¯çš„æ¢¯åº¦å˜å¾—å¾ˆå°ã€‚
é™¤ä»¥sqrt(d/h)ï¼Œç›¸å½“äºå¯¹è¿‡å¤§çš„dåŠ å…¥äº†çº¦æŸã€‚

å®Œæ•´ç»“æ„å¦‚ä¸‹

![](pic/2021-04-20-08-12-51.png)


encoderç”± 6 å±‚ç›¸åŒçš„å±‚ç»„æˆï¼Œæ¯ä¸€å±‚åˆ†åˆ«ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š

ç¬¬ä¸€éƒ¨åˆ†æ˜¯ multi-head self-attention
ç¬¬äºŒéƒ¨åˆ†æ˜¯ position-wise feed-forward networkï¼Œæ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆrelu)
ä¸¤ä¸ªéƒ¨åˆ†ï¼Œéƒ½æœ‰ä¸€ä¸ªæ®‹å·®è¿æ¥(residual connection)ï¼Œç„¶åæ¥ç€ä¸€ä¸ª Layer Normalizationã€‚

Decoder
å’Œ encoder ç±»ä¼¼ï¼Œdecoder ä¹Ÿæ˜¯ç”±6ä¸ªç›¸åŒçš„å±‚ç»„æˆï¼Œæ¯ä¸€ä¸ªå±‚åŒ…æ‹¬ä»¥ä¸‹3ä¸ªéƒ¨åˆ†:

ç¬¬ä¸€ä¸ªéƒ¨åˆ†æ˜¯ multi-head self-attention mechanism
ç¬¬äºŒéƒ¨åˆ†æ˜¯ multi-head context-attention mechanism
ç¬¬ä¸‰éƒ¨åˆ†æ˜¯ä¸€ä¸ª position-wise feed-forward network

ontext-attention æ˜¯ encoder å’Œ decoder ä¹‹é—´çš„ attentionï¼Œæ˜¯ä¸¤ä¸ªä¸åŒåºåˆ—ä¹‹é—´çš„attentionï¼Œä¸æ¥æºäºè‡ªèº«çš„ self-attention ç›¸åŒºåˆ«ã€‚

Batch Normæ–¹æ³•ç»è¿‡è§„èŒƒåŒ–å’Œç¼©æ”¾å¹³ç§»ï¼Œå¯ä»¥ä½¿è¾“å…¥æ•°æ®ï¼Œé‡æ–°å›åˆ°éé¥±å’ŒåŒºï¼Œè¿˜å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼šæ§åˆ¶æ¿€æ´»çš„é¥±å’Œç¨‹åº¦ï¼Œæˆ–æ˜¯éé¥±å’Œå‡½æ•°æŠ‘åˆ¶ä¸æ¿€æ´»çš„èŒƒå›´ã€‚è¿™ä¸ªæ€§è´¨ï¼Œä¹Ÿè§£é‡Šäº†ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šBatch Normä¸ºä»€ä¹ˆå¾€å¾€æ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚

å› ä¸ºæ¯ä¸ªæ ·æœ¬çš„åŸå§‹å¥å­çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œéœ€è¦Paddingåˆ°ç›¸åŒé•¿åº¦ï¼Œè¡¥å…¨çš„ä½ç½®ä¸Šçš„embeddingæ•°å€¼è‡ªç„¶å°±æ˜¯0äº†ã€‚

å¯¹äºé‚£äº›è¡¥é›¶çš„æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬çš„attentionæœºåˆ¶ä¸åº”è¯¥æŠŠæ³¨æ„åŠ›æ”¾åœ¨è¿™äº›ä½ç½®ä¸Šï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›å¤„ç†ã€‚å…·ä½“çš„åšæ³•æ˜¯ï¼ŒæŠŠè¿™äº›ä½ç½®çš„å€¼åŠ ä¸Šä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°(è´Ÿæ— ç©·)ï¼Œè¿™æ ·ç»è¿‡softmaxåï¼Œè¿™äº›ä½ç½®çš„æƒé‡å°±ä¼šæ¥è¿‘0ã€‚Transformerçš„padding maskå®é™…ä¸Šæ˜¯ä¸€ä¸ªå¼ é‡ï¼Œæ¯ä¸ªå€¼éƒ½æ˜¯ä¸€ä¸ªBooleanï¼Œå€¼ä¸ºfalseçš„åœ°æ–¹å°±æ˜¯è¦è¿›è¡Œå¤„ç†çš„åœ°æ–¹ã€‚
åœ¨batché¢„å¤„ç†å, tokenizerä¼šè¿”å›ä¸€ä¸ªmaskå¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä¼ é€’ç»™transformerå‡½æ•°ã€‚

# 3.3 transformeré—®é¢˜
transformeræ²¡æœ‰è§£å†³è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ï¼Œself attentionè®¡ç®—å¤æ‚åº¦æ˜¯O(L^2), Læ˜¯åºåˆ—é•¿åº¦

ä¸ºä»€ä¹ˆä½¿ç”¨ä¸ä¸€æ ·çš„Wq, Wkï¼Ÿ
ä¸€ç§è§£é‡Šæ˜¯ä¸ºäº†é¿å…attentionçŸ©é˜µå¯¹ç§°ï¼Œä½†æ˜¯å®é™…ä¸Šç»è¿‡softmaxä»¥åå¹¶ä¸ä¼šå¯¹ç§°, è™½ç„¶simaliar(i,j) == simaliar(j,i)ï¼Œä½†æ˜¯å½’ä¸€åŒ–ä»¥åå°±ä¸ç›¸ç­‰äº†ï¼ˆå› ä¸ºsoftmaxæ˜¯row-wiseçš„ï¼Œä¹Ÿå°±æ˜¯æŒ‰è¡Œå½’ä¸€åŒ–ï¼‰ï¼Œæ‰€ä»¥è¿™ç§è¯´æ³•ä¸æ­£ç¡®ã€‚
è™½ç„¶æ˜¯è¿™æ ·çš„ï¼Œä½†æ˜¯simaliarçŸ©é˜µçš„å¯¹ç§°æ€§ï¼Œä¹Ÿæ˜¯ç›¸å½“äºç»™ç›¸ä¼¼åº¦åŠ äº†ä¸€ä¸ªæ­£åˆ™åŒ–çš„çº¦æŸï¼Œé™ä½äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸ä¼¼åº¦çŸ©é˜µä¹Ÿå°±æ˜¯å†…ç§¯çŸ©é˜µã€‚



softmaxå€¼å¾—æ˜¯softç‰ˆæœ¬çš„max, å› æ­¤softmaxå®é™…ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒæ±‚å’Œï¼Œå½“ç„¶æœ€åçš„ç»“æœä¸­ï¼Œæœ€å¤§çš„å€¼å æ®æœ€ä¸»è¦éƒ¨åˆ†ã€‚hardmaxä¸å¯å¯¼ï¼Œæ‰€ä»¥softmaxæ‰è¢«é‡‡ç”¨ï¼Œ
softmaxç›´ç™½æ¥è¯´å°±æ˜¯å°†åŸæ¥è¾“å‡ºæ˜¯3,1,-3é€šè¿‡softmaxå‡½æ•°ä¸€ä½œç”¨ï¼Œå°±æ˜ å°„æˆä¸º(0,1)çš„å€¼ï¼Œè€Œè¿™äº›å€¼çš„ç´¯å’Œä¸º1ï¼ˆæ»¡è¶³æ¦‚ç‡çš„æ€§è´¨ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒç†è§£æˆæ¦‚ç‡ã€‚




# tokenizer

tokenizeå°±æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯+è¯è¯­æ•°å€¼åŒ–.
SentencePieceä»¥åŠWordPieceæ˜¯ä¸¤ç§tokeinzationæ–¹æ³•.
SentencePieceä¸€èˆ¬åŸºäºBPEæˆ–è€…ULMï¼Œå…¶ä¸­BPEæ˜¯åŸºäº2-gramsé¢‘æ¬¡æ„å»ºè¯è¡¨ï¼ŒULMæ˜¯åŸºäºunigram language modelï¼ˆä¸€å…ƒè¯­è¨€æ¨¡å‹ï¼‰æ„å»ºè¯è¡¨ã€‚WordPieceä¹Ÿæ˜¯åŸºäºè¯­è¨€æ¨¡å‹æ„å»ºè¯è¡¨çš„ã€‚


BPEï¼ˆByte Pair Encodingï¼‰å°±æ˜¯ç”¨æ¥æ„å»ºè¯è¡¨çš„ã€‚æ€ä¹ˆæ„å»ºï¼Œå½“ç„¶å°±æ˜¯æ–°è¯å‘ç°äº†ï¼Œè€Œä¸”è¿˜ç”¨çš„æœ€ç®€å•çš„ä¸€ç§æ–¹æ³•ï¼Œé¢‘æ¬¡ã€‚æ€æƒ³å°±æ˜¯æ‰“ç ´è¯è¯­å•å…ƒï¼Œæƒ³è¦å¼•å…¥æ›´ç»†èŠ‚çš„ä¿¡æ¯,å› æ­¤ä»å­—ç¬¦å±‚æ¬¡å¼€å§‹æå–ã€‚

1. å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ‡åˆ†ã€‚
2. å°†åˆ†è¯åçš„æ¯ä¸ªå•è¯è¿›è¡Œè¿›ä¸€æ­¥åˆ‡åˆ†ï¼Œåˆ’åˆ†ä¸ºå­—ç¬¦åºåˆ—ã€‚ã€‚åŒæ—¶ï¼Œåœ¨æ¯ä¸ªå•è¯ç»“å°¾æ·»åŠ ç»“æŸç¬¦ï¼Œä»¥ä¿ç•™å•è¯è¾¹ç•Œä¿¡æ¯ï¼ˆå› ä¸ºä¸‹ä¸€æ­¥ç»Ÿè®¡2-gramsé¢‘æ¬¡æ—¶ï¼Œä¸å…è®¸è·¨è¯è¾¹ç•Œæ„æˆ2-gramsï¼‰
3. ç»Ÿè®¡æ¯ä¸ªå•è¯ä¸­2-gramsä¸²å‡ºç°çš„é¢‘æ¬¡ï¼Œé€‰æ‹©top-2çš„2-gramsä¸²ï¼Œå°†å…¶ä½œä¸ºæ–°è¯æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚
4. ä¸æ–­é‡å¤ä¸Šä¸€æ­¥ï¼Œç›´åˆ°è¯è¡¨å¤§å°è¾¾åˆ°é¢„è®¾çš„å¤§å°ï¼Œæˆ–è€…æ–°è¯çš„é¢‘æ¬¡ç­‰äº1ã€‚

æ„å»ºå®Œè¯åå°±è¿›è¡Œæ•°å€¼åŒ–ã€‚
åœ¨è¿™é‡Œä¸­æ–‡æ€ä¹ˆåšæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æŒ‘æˆ˜ã€‚

https://zhuanlan.zhihu.com/p/267144841

# RNNé—®é¢˜
Problem:RNNs take O(sequence length)steps for distant word pairs to interact.
Linear interaction distance.

1. Hard to learn long-distance dependencies (because gradient problems!)
2. linear order isnâ€™t the right way to think about sentences...6Thewaschefwho  ...Info of chefhas gone through O(sequence length) many layers!
3. Lack of parallelizability.Inhibits training on very large datasets!

æ­¤æ—¶éœ€è¦ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œæ¯”å¦‚
Word window models aggregate local contexts(1d conv),
Stacking word window layers allows interaction between farther words.

![](pic/2021-04-17-11-42-14.png)

1d cnn, Maximum Interaction distance = sequence length / window sizeï¼Œ è¿™é‡Œçš„æœ€å¤§äº¤äº’è·ç¦»æŒ‡ï¼Œæœ€è¿œçš„ä¸¤ä¸ªè¯ï¼Œåœ¨æ¨¡å‹ä¸­å‘ç”Ÿäº¤äº’ï¼Œéœ€è¦çš„è®¡ç®—é•¿åº¦ï¼Œæœ€è¿œçš„è¯å°±åœ¨æœ€è¿œçš„ä¸¤ä¸ªæ—¶é—´çª—å£ï¼Œè·ç¦»ä¹Ÿå°±æ˜¯æ—¶é—´çª—å£çš„æ•°é‡äº†

# attention for time-series

attentionä¹Ÿæ˜¯ä¸€ä¸ªé€‰é¡¹ã€‚
Attention treats each wordâ€™s representation as a queryto access and incorporate information from a set of valuesã€‚
Maximum interaction distance: O(1), since all words interact at every layer!


![](pic/2021-04-17-11-58-26.png)

ä½†æ˜¯attentionçš„é—®é¢˜ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œself-attentionæœ¬èº«æ˜¯æ²¡æœ‰ä½ç½®ä¿¡æ¯çš„ã€‚
å› æ­¤éœ€è¦Postion embedding.
æœ€ç®€å•çš„ä½ç½®ç¼–ç å°±æ˜¯è®¡æ•°ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ¡ˆé—®é¢˜å¾ˆå¤§ï¼Œå¾ˆé•¿çš„(æ¯”å¦‚å«æœ‰500ä¸ªå­—çš„)æ–‡æœ¬ï¼Œæœ€åä¸€ä¸ªå­—çš„ä½ç½®ç¼–ç éå¸¸å¤§ã€‚å®ƒæ¯”ä¸€èˆ¬çš„å­—åµŒå…¥çš„æ•°å€¼è¦å¤§ï¼Œéš¾å…ä¼šæŠ¢äº†å­—åµŒå…¥çš„ã€Œé£å¤´ã€ï¼Œå¯¹æ¨¡å‹å¯èƒ½æœ‰ä¸€å®šçš„å¹²æ‰°ã€‚



ä½ç½®ç¼–ç æœ€å¥½å…·æœ‰ä¸€å®šçš„å€¼åŸŸèŒƒå›´ï¼Œæˆ‘ä»¬å…³æ³¨çš„ä½ç½®ä¿¡æ¯ï¼Œæœ€æ ¸å¿ƒçš„å°±æ˜¯ç›¸å¯¹æ¬¡åºå…³ç³»ã€‚å¦‚æœæˆ‘ä»¬å¯¹ä½ç½®å½’ä¸€åŒ–çš„è¯ï¼Œé•¿çŸ­æ–‡æœ¬çš„ç›¸é‚»å•è¯ä½ç½®ç¼–ç ä¿¡æ¯å·®å¼‚å·¨å¤§

ä½ç½®ç¼–ç çš„éœ€æ±‚ï¼š1. éœ€è¦ä½“ç°åŒä¸€ä¸ªå•è¯åœ¨ä¸åŒä½ç½®çš„åŒºåˆ«ï¼›2. éœ€è¦ä½“ç°ä¸€å®šçš„å…ˆåæ¬¡åºå…³ç³»ï¼Œå¹¶ä¸”åœ¨ä¸€å®šèŒƒå›´å†…çš„ç¼–ç å·®å¼‚ä¸åº”è¯¥ä¾èµ–äºæ–‡æœ¬é•¿åº¦ï¼Œå…·æœ‰ä¸€å®šä¸å˜æ€§ã€‚æˆ‘ä»¬æ”¾å¼ƒå¯¹ç»å¯¹ä½ç½®çš„è¿½æ±‚ï¼Œè½¬è€Œè¦æ±‚ä½ç½®ç¼–ç ä»…ä»…å…³æ³¨ä¸€å®šèŒƒå›´å†…çš„ç›¸å¯¹æ¬¡åºå…³ç³»ï¼Œé‚£ä¹ˆä½¿ç”¨ä¸€ä¸ªsin/coså‡½æ•°å°±æ˜¯å¾ˆå¥½çš„é€‰æ‹©.

ä¸€ç§æ€è·¯æ˜¯ä½¿ç”¨æœ‰ç•Œçš„å‘¨æœŸæ€§å‡½æ•°ã€‚
å¦‚ä½•ç†è§£Transformerè®ºæ–‡ä¸­çš„positional encodingï¼Œå’Œä¸‰è§’å‡½æ•°æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ - TniLçš„å›ç­” - çŸ¥ä¹
https://www.zhihu.com/question/347678607/answer/864217252

éœ€è¦å°†ä½ç½®ä¿¡æ¯åµŒå…¥åˆ°è¾“å…¥ä¸­ï¼Œä½†æ˜¯åµŒå…¥çš„ä½ç½®ä¿¡æ¯ä¸èƒ½å–§å®¾å¤ºä¸»ï¼Œä¸èƒ½å½±å“åŸå§‹è¾“å…¥ã€‚
ä½ç½®ç¼–ç çš„éœ€æ±‚ï¼š1. éœ€è¦ä½“ç°åŒä¸€ä¸ªå•è¯åœ¨ä¸åŒä½ç½®çš„åŒºåˆ«ï¼›2. éœ€è¦ä½“ç°ä¸€å®šçš„å…ˆåæ¬¡åºå…³ç³»ï¼Œå¹¶ä¸”åœ¨ä¸€å®šèŒƒå›´å†…çš„ç¼–ç å·®å¼‚ä¸åº”è¯¥ä¾èµ–äºæ–‡æœ¬é•¿åº¦ï¼Œå…·æœ‰ä¸€å®šä¸å˜æ€§ã€‚3. å€¼åŸŸè¦æœ‰é™åˆ¶ï¼Œæœ€å¥½åœ¨[0,1]


å‰æœ‰ä¸‰ç§ä¸»æµçš„æŠ€æœ¯å¯ä»¥è§£å†³ï¼š

ç”¨æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Position Encodingï¼‰
å­¦ä¹ ä½ç½®å‘é‡ï¼ˆç±»ä¼¼è¯å‘é‡ï¼‰
ç›¸å¯¹ä½ç½®è¡¨è¾¾ï¼ˆRelative Position Representationsï¼‰

transformeré‡‡ç”¨äº†Sinusoidal position representationsï¼Œ è¿™ä¸ªå…¬å¼æ˜¯æ¯”è¾ƒç„å­¦çš„ï¼Œæ•ˆæœè·Ÿåˆ«çš„å·®åˆ«ä¹Ÿä¸å¤§ï¼Œä¸ç”¨å¤ªåœ¨æ„ã€‚
![](pic/2021-04-17-12-41-43.png)

attentionç”¨äºæ—¶åºæ•°æ®éœ€è¦è€ƒè™‘ä¸‹é¢ä¸‰ä¸ªé—®é¢˜
1. position learned from scratch

æ€æƒ³ç±»ä¼¼è¯å‘é‡

![](pic/2021-04-17-12-43-22.png)

   * Sometimes people try more flexible representations of position:â€¢Relative linear position attention [Shaw et al., 2018ã€‘
   * Dependency syntax-based position [Wang et al., 2019]Position representation vectors learned from scratch16

2. Adding nonlinearities in self-attention

   * stacking more self-attention layers just re-averages valuevectors.
  * Easy fix: add a feed-forward network to post-process each output vector

3. Need to ensure we donâ€™t â€œlook at the futureâ€ when predicting a sequence
   * machine translation and language modeling
   * Masking the future in self-attention of decoder
   * At every timestep, we could change the set of keys and queriesto include only past words. (Inefficient!)
   * To enable parallelization, we mask out attention to future words by setting attention scores to æ— ç©·å¤§

![](pic/2021-04-17-20-42-59.png)





# 3. transformer

![](pic/2021-04-18-07-19-38.png)


ä¸Šé¢å¯¹attentionçš„ä¼˜åŒ–è¿˜ä¸å¤Ÿ, Transformerè¿˜åšäº†ä¸‹é¢çš„ä¸€äº›æ”¹åŠ¨
1. KQVå½¢å¼çš„self-attentionï¼Œä¹‹å‰ç”¨çš„attentionè¿˜æ˜¯å¤ªç®€å•äº†
2. Multi-headed attention: Attend to multiple places in a single layer!
3. training trick
   * Residual connections2
   * Layer normalization3
   * Scaling the dot product

## 3.1 attention
KVQ, attentionç”¨æ¥ä¸‰ä¸ªçŸ©é˜µï¼Œå¯¹åŸå§‹è¾“å…¥è¿›è¡Œå˜æ¢ï¼Œæœ€åè®¡ç®—attentionã€‚
These matrices allow different aspectsof the ğ‘¥vectors to be used/emphasized in each of the three roles.

![](pic/2021-04-18-09-01-47.png)

ä¸Šé¢çš„KQVæŒ‡çš„éƒ½æ˜¯å˜æ¢çŸ©é˜µã€‚

ç±»æ¯”CNNä¸­åŒæ—¶ä½¿ç”¨å¤šä¸ªæ»¤æ³¢å™¨çš„ä½œç”¨ï¼Œç›´è§‚ä¸Šè®²ï¼Œå¤šå¤´çš„æ³¨æ„åŠ›æœ‰åŠ©äºç½‘ç»œæ•æ‰åˆ°æ›´ä¸°å¯Œçš„ç‰¹å¾/ä¿¡æ¯ã€‚

For word ğ‘–ï¼Œmaybe we want to focus on different j for different reasons? Weâ€™ll define multiple attention â€œheadsâ€ through multiple Q,K,V matrices.
![](pic/2021-04-18-09-49-03.png)

multi-head attentionå…¶å®ç›¸å½“äºå°†Q,K,Vå†è¿›ä¸€æ­¥æ‹†åˆ†æˆhä¸ªç‹¬ç«‹çš„çŸ©é˜µï¼Œæ¯ä¸ªçŸ©é˜µå¯¹åŸå§‹è¾“å…¥å˜æ¢åˆ°ä¸åŒçš„ç©ºé—´ä¸­ï¼Œæœ€åå°†å…¶æ‹¼æ¥ã€‚
Each head gets to â€œlookâ€ at different things, and construct value vectors differently.
ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Œä¹Ÿå°±æ˜¯ç»¼åˆåˆ©ç”¨å„æ–¹é¢çš„ä¿¡æ¯/ç‰¹å¾ã€‚
æ¯”å¦‚å½“ä½ æµè§ˆç½‘é¡µçš„æ—¶å€™ï¼Œä½ å¯èƒ½åœ¨é¢œè‰²æ–¹é¢æ›´åŠ å…³æ³¨æ·±è‰²çš„æ–‡å­—ï¼Œè€Œåœ¨å­—ä½“æ–¹é¢ä¼šå»æ³¨æ„å¤§çš„ã€ç²—ä½“çš„æ–‡å­—ã€‚
![](pic/2021-04-18-09-53-40.png)

å¤šå¤´æ³¨æ„åŠ›çš„æœºç†è¿˜ä¸æ˜¯å¾ˆæ¸…æ¥šã€‚äº‹å®ä¸Šï¼Œæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«å¦‚ä½•å·¥ä½œï¼Œè¿™ä¸ªå¯è§£é‡Šæ€§å·¥ä½œä¹Ÿè¿˜æ²¡æœ‰å®Œæˆï¼Œç›®å‰çš„ä¸€äº›è§£é‡Šéƒ½è¿˜åªæ˜¯intuitionã€‚

multi-headå¯¹encoder-decoder attentionæå‡ä¸å°ï¼Œä½†å¯¹self-attentionç›®å‰çœ‹æé«˜å¾ˆæœ‰é™ã€‚NMTä»»åŠ¡ä¸Šheadså¤§äº4å°±æ²¡ä»€ä¹ˆæé«˜äº†ã€‚
ä¸ªäººæ„Ÿè§‰è¿™å¯èƒ½ç±»ä¼¼äº CNN æ¨¡å‹ä¸­å¤š channel çš„ä¸€ç§ä½œç”¨ã€‚

ä¸ºäº†æé«˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œéœ€è¦è¶³å¤Ÿçš„æ¨¡å‹å®¹é‡ï¼Œä¹Ÿå°±éœ€è¦hidden sizeè¶³å¤Ÿå¤§ã€‚ä½†æ˜¯å¦‚æœä¸æ‹†åˆ†æˆå¤šå¤´ï¼Œé‚£ä¹ˆæ¨¡å‹å°±è¿‡äºå¤æ‚äº†ã€‚æ‰€ä»¥å¤šå¤´çš„å¤§hidden sizeå®é™…ä¸Šå°±æ˜¯å¤æ‚åº¦å’Œå®¹é‡çš„å¹³è¡¡ã€‚å¤´æ•°çš„å¥½å¤„å°±æ˜¯ä¸ºå¹³è¡¡æ¨¡å‹æä¾›äº†ä¸€ä¸ªå¤šä½™çš„è°ƒå‚ç©ºé—´ã€‚è¿™ä¹Ÿè§£é‡Šäº†å¤´æ•°ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œä¹Ÿä¸æ˜¯è¶Šå°‘è¶Šå¥½ï¼Œè€Œæ˜¯éœ€è¦å¹³è¡¡ã€‚

Residual connections are a trick to help models train better.
Residualæœ¬è´¨æ˜¯æ‰“ç ´äº†å±‚çº§ç»“æ„çš„æ˜¾ç¤ºï¼Œä½¿å¾—ä¸‹å±‚ä¿¡æ¯æ›´å®¹æ˜“ä¼ é€’åˆ°ä¸Šå±‚ã€‚
Residual connections are thought to makethe loss landscape considerably smoother(thuseasier training!)

![](pic/2021-04-18-10-12-20.png)


## 3.2 SScaled Dot Product
![](pic/2021-04-18-19-24-21.png)

* When dimensionality ğ‘‘ becomes large, dot products between vectors tend to become large.
* Because of this, inputs to the softmaxfunction can be large, making the gradients small
* We divide the attention scores by sqrt(d/h), to stop the scores from becoming large just 


å®Œæ•´ç»“æ„å¦‚ä¸‹

![](pic/2021-04-18-20-10-55.png)
![](pic/2021-04-18-20-10-40.png)



cross attentionçš„è¯¦ç»†å…¬å¼å¯ä»¥ç”¨ä¸‹é¢è¡¨ç¤º
![](pic/2021-04-18-20-15-34.png)

Transformersâ€™ parallelizability allows for efficient pretraining, andhave made them the de-facto standard

## transformeræ”¹è¿›ç©ºé—´
1. Quadratic compute in self-attention
    * self attentionç®—æ³•å¤æ‚åº¦O(T^2d), computation grows quadraticallywith the sequence length!
    * Linformer, map the sequence length dimension to a lower-dimensional space for values, keys
    * replace all-pairs interactions with a family of other interactions, like local windows, looking at everything, and random interactions.

2. Position representations
    * Dependency syntax-based position
    * Relative linear position attention(Self-Attention with Structural Position Representations, Tencent AI Lab)




KQV