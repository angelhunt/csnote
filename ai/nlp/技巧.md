这个问题现在争议不大了吧，我自己遵守的一条好用的黄金法则是有大量数据，用transformer；允许pretrain，用transformer；小数据from scratch，分类用CNN，标注和生成用RNN。对于上述情况的中间地带，看个人调参能力吧，个人实践经验来说，哪个调的用心哪个就更好一些，喜欢无脑套默认超参的还是RNN更容易上性能，不过是真的慢。

如果你的序列具有极其严格的时序性依赖关系（比如某些重排序任务）的话，基于位置编码的Transformer Encoder是没有办法很好地捕获这些时序关系的。这种情况下要找到这样的时序性关系，你还是得用到RNN。

当序列长度没有超过RNN的处理能力的时候，位置编码对时序性的建模能力跟RNN显然是没得比的。ACL的哪篇文章了，做过一个实验。RNN对于50个词之前的词顺序就不敏感了，而对于100个词之前的就完全忘了，