# 总结
最大熵原理：学习概率模型的时候，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。

逻辑回归是最大熵对应广义线性模型在二类时的特殊情况，也就是说，广义线性模型假设当给定x和w时，随机变量y的分布服从某个指数族分布，而指数族函数是在满足某些统计量约束，例如均值方差固定时，熵最大的模型。
https://sm1les.com/2019/01/13/exponential-family-and-maximum-entropy

当逻辑回归扩展为多类别的时候，就是最大熵模型。


优点
1. 本身算法的计算代价不高，对时间和内存需求较小, 求解梯度的算法都非常成熟，整体需要的资源较小，所以适合简单的线性问题和在线推荐系统
2. 使用梯度下降的优化算法可以用于在线算法实现和分布式系统
3. LR对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响


缺点
1. 本质是线性模型，模型准确性不高，对于复杂问题需要海量的数据。
2. 容易受到严重的多重共线性特征影响，也就是说容易受到彼此相关性较高的特征的影响，使用时需要对数据进行相关性检测，处理掉耦合性较高的特征。
3. 对于数值特征, 在海量数据下, 数值数据抖动非常大, 拟合困难，影响模型整体性能。所以比较适合离散特征
常见的应用场景：



1. 为什么使用正则化?
  因为使用极大似然估计,模型会全力拟合数据,容易受到脏数据和异常点的影响
2. 为什么一般使用L2正则化?
  因为L2正则化只会使函数的某些参数缩小,降低这些参数的作用. 但是如果直接使用L1正则化会使参数直接为0, 会极大降低模型的效果. 所以一般我们选择更温和的L2正则化.
3. 为什么要使用集成的决策树模型，而不是单棵的决策树模型?
  一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。可以更好的发现有效的特征和特征组合.
4. 除了GBDT+LR的方案,还有哪些思路可以挖掘有效的特征组合?
  类似地, 其他树+LR, 其他树+FM, 但是其他树+FM的效果在比赛中效果不好。
5. 通过GBDT映射得到的特征空间维度如何？
  GBDT树有多少个叶子节点，通过GBDT得到的特征空间就有多大。假设GBDT有n棵树，每颗树至少有m个叶子，得到的特征空间是大小是m*n,如果更多的树模型，特征空间将成指数上升。

# 应用

LGB和LR的融合方案:有两种用法：1. 用离散特征训练树，用树的节点位置做特征 2. 建立非ID和ID树

LGB适合处理连续数据，可以在模型内部将连续数据分桶，得到的离散特征具有良好的鲁棒性。LGB模型的predict_leaf_index可以直接输出样本在不同树上的位置，不同的位置表示不同的特征组合。基于树的方法避免人工特征的繁琐、低效率和低准确率。
商品ID或者广告ID数量庞大，如果直接训练树容易产生畸形树，所以只能是少量ID+大量特征或者大量ID+少量特征，保证整棵树的平衡有效。
非ID树：针对所有广告，包括没有曝光的广告建立非ID树，利用大量广告本身的特征形成对长尾广告的预测
ID树：针对不同的ID特征建立不同的ID树，以ID相关特征形成普通广告的预测。
结合两种树的结果，精心调整不同树的权重，提高CTR的预估。
关于ID树和非ID树的详细介绍，可以参考这篇文章


# 模型公式

![](pic/2021-03-18-21-31-11.png)

![](pic/2021-03-18-21-31-52.png)

线性模型 [公式] 往往用来预测的是连续值，对于分类问题效果往往不好，故考虑再对预测值做一次处理，这个处理的函数称之为连接函数（link function） 。


为什么试用sigmoid

sigmoid，或者说exponential family所具有的最佳性质，即maximum entropy的性质。

我们现在关心的是，给定某些假设之后，熵最大的分布。也就是说这个分布应该在满足我假设的前提下越均匀越好。比如大家熟知的正态分布，正是假设已知mean和variance后熵最大的分布。

对于逻辑回归
我们在建模预测 Y|X，并认为 Y|X 服从bernoulli distribution，所以我们只需要知道 P(Y|X)；其次我们需要一个线性模型，所以 P(Y|X) = f(wx)。接下来我们就只需要知道 f 是什么就行了。而我们可以通过最大熵原则推出的这个 f，就是sigmoid。

因为bernoulli的指数族分布形式下的，最大熵结果就是sigmoid

https://www.zhihu.com/question/35322351/answer/67193153


Q：为什么要使用指数族分布？

A：因为指数族分布是给定某些统计量下熵最大的分布，例如伯努利分布就是只有两个取值且给定期望值为 [公式] 下的熵最大的分布。

Q：为什么要使用熵最大的分布？

A：最大熵理论


# 求解
优化逻辑回归的方法有非常多。
这里只谈谈梯度下降，牛顿法和BFGS

这里只说梯度下降

先定义逻辑回归的损失函数, 逻辑回归采用对数似然损失函数

L(Y,P(Y∣X))=−logP(Y∣X)

![](pic/2021-03-18-22-42-58.png)

因为是连加，我们只需要，求导求和公式内部，然后再将导数求和。

![](pic/2021-03-19-08-37-04.png)
上面的公式写错了，应该是y-sigmoid(xi),因为作者忘记乘一开始提取的常数项了，常数项是负数，因此要取反

也可以用向量表示，直接就是sum(sigmoid(x) -y) * x

https://laobadao.github.io/2017/10/27/logistic-cost/index.html




Sklearn中自带算法中两个特别的点:

梯度下降法实现相对简单，但是其收敛速度往往不尽人意。所以在sklearn中LR用到的是sag、saga（这两种是梯度下降法的改进）、liblinear、cg（共轭梯度法，最快的梯度下降法）和lbfgs（拟牛顿法的改进）
最大似然估计法没有考虑训练集以外的因素，很容易造成过拟合，为了解决过拟合问题，通过添加正则化项，控制模型的复杂程度。常用的有L1和L2正则化。