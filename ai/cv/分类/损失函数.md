# 人脸识别


站在现在往回看，center loss这个设计不是很有必要。而且也不是说类内缩得越小越好，后面有了margin，发现margin也不是越大越好。

我这回答的意思就是说：方法可能已经被更新了，但分析的手段却流传了下来

其实是因为做的是个度量学习的任务，softmax只是用来训练特征，测试的时候softmax和之前的fc都是要扔掉的。

特征和特征之间在比对的时候，如果仍然有模长的因素在里面（也就是内积），那一个特别长的特征，与另一类也比较长的特征相乘，是有可能比自己类别里一长一短的特征更大的，所以测试时要做归一化。

那么为了与测试一致，训练时怎么能不做L2归一化呢？


测试的时候，只用feature，不要概率。
通过feature之间的角度作为相似度。

## center loss

散射状分布是因为距离度量用的是内积，换成cosine就是球面分布，L1/2就是点状聚集。

而且我觉得这可能误导了一些工作，之前有篇center loss我觉得就很奇怪。

虽然散射状分布看起来是不够紧密， 但本质上是因为在内积这个metric下，紧密的分布就是长这样。

非要加一个L2约束让分布变成个球我觉得没什么意义。

后面也没什么人follow。


## margin 

AM softmax是为度量学习设计的。

