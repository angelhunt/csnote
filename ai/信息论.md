精选文章 干货 | 对数线性模型之 Logistic 回归、SoftMax 回归和最大熵模型
干货 | 对数线性模型之 Logistic 回归、SoftMax 回归和最大熵模型
作者：机器学习算法那些事 时间: 2021-02-05 09:56:37
标签：ffmpeg获取视频信息获取文件格式信息
【摘要】本文介绍对数线性分类模型，在线性模型的基础上通过复合函数（sigmoid，softmax，entropy ）将其映射到概率区间，使用对数损失构建目标函数。首先以概率的方式解释了logistic回归为什么使用sigmoid函数和对数损失，然后将二分类扩展到多分类，导出sigmoid函数的高维形式softmax函数对应softmax回归，最后最大熵模型可以看作是softmax回归的离散型版本，l...
本文介绍对数线性分类模型，在线性模型的基础上通过复合函数（sigmoid，softmax，entropy ）将其映射到概率区间，使用对数损失构建目标函数。首先以概率的方式解释了logistic回归为什么使用sigmoid函数和对数损失，然后将二分类扩展到多分类，导出sigmoid函数的高维形式softmax函数对应softmax回归，最后最大熵模型可以看作是softmax回归的离散型版本，logistic回归和softmax回归处理数值型分类问题，最大熵模型对应处理离散型分类问题。

作者 | 文杰

编辑 | yuquanle

Logistic回归

A、Logistic回归


分类问题可以看作是在回归函数上的一个分类。一般情况下定义二值函数，然而二值函数构成的损失函数非凸，一般采用sigmoid函数平滑拟合（当然也可以看作是一种软划分，概率划分）：从函数图像我们能看出，该函数有很好的特性，适合二分类问题。至于为何选择Sigmoid函数，后面可以从广义线性模型导出为什么是Sigmoid函数。

逻辑回归可以看作是在线性回归的基础上构建的分类模型，理解的角度有多种（最好的当然是概率解释和最小对数损失），而最直接的理解是考虑逻辑回归是将线性回归值离散化。即一个二分类问题（二值函数）如下：



Sigmoid函数：

干货 | 对数线性模型之 Logistic 回归、SoftMax 回归和最大熵模型1

0-1损失的二分类问题属于一种硬划分，即是与否的划分，而sigmoid函数则将这种硬划分软化，以一定的概率属于某一类（且属于两类的加和为1）。

Sigmoid函数将线性回归值映射到 的概率区间，从函数图像我们能看出，该函数有很好的特性，适合二分类问题。 因此逻辑回归模型如下：


这里对于目标函数的构建不再是最小化函数值与真实值的平方误差了，按分类原则来讲最直接的损失因该是0-1损失，即分类正确没有损失，分类错误损失计数加1。但是0-1损失难以优化，存在弊端。结合sigmoid函数将硬划分转化为概率划分的特点，采用概率的对数损失（概率解释-N次伯努利分布加最大似然估计），其目标函数如下：

同样采用梯度下降的方法有：

又：

所以有：


B、概率解释

逻辑回归的概率解释同线性回归模型一致，只是假设不再是服从高斯分布，而是服从0-1分布，由于 ，假设随机变量y服从伯努利分布是合理的 。即：

所以最大化似然估计有：

logistic采用对数损失（对数似然函数）原因：

1) 从概率解释来看，多次伯努利分布是指数的形式。由于最大似然估计导出的结果是概率连乘，而概率（sigmoid函数）恒小于1，为了防止计算下溢，取对数将连乘转换成连加的形式，而且目标函数和对数函数具备单调性，取对数不会影响目标函数的优化值。

2）从对数损失目标函数来看，取对数之后在求导过程会大大简化计算量。


Softmax回归


A、Softmax回归

Softmax回归可以看作是Logistic回归在多分类上的一个推广。考虑二分类的另一种表示形式：

当logistic回归采用二维表示的话，那么其损失函数如下：

其中，在逻辑回归中两类分别为，二在softmax中采用，两个随机变量组成二维向量表示，当然隐含约束.为了更好的表示多分类问题，将（不一定理解为的取值为，更应该理解为可以取类）多分类问题进行如下表示：

其中向量的第位为1，其他位为，也就是当 时将其映射成向量时对应第位为。采用多维向量表示之后，那么对于每一维就变成了一个单独的二分类问题了，所以softmax函数形式如下：

其中函数值是一个维的向量，同样采用对数损失（N元伯努利分布和最大似然估计），目标函数形式是logistic回归的多维形式。

其中表示第个样本的标签向量化后第维的取值或者.可以看出Softmax的损失是对每一类计算其概率的对数损失，而logistic回归是计算两类的回归，其本质是一样。Logistic回归和Softmax回归都是基于线性回归的分类模型，两者无本质区别，都是从伯努利分结合最大对数似然估计。只是Logistic回归常用于二分类，而Softmax回归常用于多分类。而且Logistic回归在考虑多分类时只考虑类。

概率解释(求导推导)：
二分类与多分类可以看作是二元伯努利分布到多元伯努利分布的一个推广，概率解释同Logistic回归一致。详细解释放到广义线性模型中。


B、二分类转多分类思想

对于多分类问题，同样可以借鉴二分类学习方法，在二分类学习基础上采用一些策略以实现多分类，基本思路是“拆解法”，假设N个类别 ，经典的拆分算法有“一对一”，“一对多”，“多对多”，
一对一的基本思想是从所有类别中选出两类来实现一个两分类学习器，即学习出个二分类器，然后对新样本进行预测时，对这 个分类器进行投票最终决定属于那一类。

一对多的基本思想是把所有类别进行二分类，即属于类和非两类，这样我们就需要N个分类器，然后对新样本进行预测时，与每一个分类器比较，最终决定属于哪一类。这其实就是Softmax的思想，也是SVM多分类的思想。

最大熵模型

很奇怪，为什么会把最大熵模型放到这，原因很简单，它和Logistic回归和SoftMax回归实在是惊人的相似，同属于对数线性模型。

A、熵的概念


干货 | 对数线性模型之 Logistic 回归、SoftMax 回归和最大熵模型2

信息熵：熵是一种对随机变量不确定性的度量，不确定性越大，熵越大。若随机变量退化成定值，熵为0。均匀分布是“最不确定”的分布 。

假设离散随机变量X的概率分布为，则其熵为：

其中熵满足不等式，为取值数。

联合熵：对于多个随机变量的不确定性可以用联合熵度量。

假设离散随机变量的联合概率分布为，则其熵为：

条件熵：在给定条件下描述随机变量的不确定性。

假设离散随机变量，在给定的条件下的不确定性为条件熵H(X|Y)，也就等于。

互信息：衡量两个随机变量相关性的大小。

相对熵（KL散度）：衡量对于同一个随机变量两个概率分布的差异性。

有互信息和相对熵的定义有下式：

关于熵的介绍就到此，不细究，虽然上面的这些定义在机器学习中都会遇到，不过后面涉及到的主要还是熵和条件熵，互信息。