

# fine-tune方法

LoRA, 只fine-tune部分参数, lora的优点在于轻量化，低资源。但缺点很明显，参与训练的模型参数量不多，也就百万到千万级别的参数量，所以效果比全量微调差很多。可能在扩散模型上感知没那么强，但在LLM上，个人感觉表现还是差距挺大的。
推荐qlora技术，这是一种量化的lora技术。核心在于，将base model的权重量化到4-bit，降低显存占用。在所有linear后面都插入adapter，并且提高adapter中的rank，以更多的训练参数量，弥补量化带来的精度损失。

https://www.zhihu.com/question/608674675